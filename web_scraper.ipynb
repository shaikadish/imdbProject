{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaikadish/imdbProject/blob/main/web_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are we doing here?\n",
        "\n",
        "Although [brilliant datasets](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) exist for sentiment analysis using IMDB user reviews, you might need something a little different. \n",
        "\n",
        "For example, the project I am working on is to try and estimate the rating out of ten a user would give a film, based on their review of the film. I struggled to find a dataset that had exactly what I needed (user reviews and their ratings), so I decided to make my own. This notebook has the code I used to do that.\n",
        "\n",
        "I hope that this notebook can act as a tutorial for anyone interested in building their own dataset through the use of pretty standard web-scraping techniques.\n"
      ],
      "metadata": {
        "id": "sk-cbVbHfRUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and setup\n",
        "\n",
        "Firstly, I am working off colabs, so there is some code here just to mount to your google drive, and to import the appropriate python libraries. The main libraries we will be using here are: \n",
        "*   [Selenium](https://selenium-python.readthedocs.io/) to automate web browsing activities.\n",
        "*   [requests](https://docs.python-requests.org/en/latest/) for reading the *HTML* file on a given page.\n",
        "*   [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) for parsing the *HTML* files.\n",
        "*   [Pandas](https://pandas.pydata.org/docs/) for tabular data manipulation and saving."
      ],
      "metadata": {
        "id": "2nTxApuahhxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjK7_udlZyBX",
        "outputId": "13db3415-0730-4559-d9d7-f8f5bd71a379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/GitHub/IMDB_project/imdbProject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR2TXAN0nvGL"
      },
      "outputs": [],
      "source": [
        "# Install libraries to colab environment\n",
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyN5wMyUpMKN"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import selenium\n",
        "import requests \n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnnoIS6AsiPw",
        "outputId": "ccd4b83b-323d-4b02-c68f-d4c98a181fa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: use options instead of chrome_options\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "# Configure Selenium webdriver for colab notebook\n",
        "from selenium import webdriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping\n",
        "\n",
        "Now comes the good stuff. We will get access to the review data in three steps:\n",
        "\n",
        "\n",
        "1.   This step automates the traversal of an [*IMDB list page*](https://www.imdb.com/search/title/?title_type=feature,tv_movie&release_date=2010-01-01,2022-01-01&languages=en&adult=include&sort=release_date,asc&start=0&ref_=adv_nxt), where the set of films can be found. The URL to each of these films [*IMDB page*](https://www.imdb.com/title/tt0368226/?ref_=nv_sr_srsg_0) are extracted through this traversal.\n",
        "2.   Next, each of these film's *IMDB Page*s are visited, and the URL to that film's [review page](https://www.imdb.com/title/tt0368226/reviews?ref_=tt_urv) is scraped.\n",
        "3.   Finally, the user review and ratings are scraped from each of review page.\n",
        "\n",
        "This process is broken into these steps because it is less convoluted to automate each process individually than to try do them all at once. This approach was also used for those of you are here to copy some code, as each of these steps can easily be modified for similar projects, abd the modularity helps with code editing.\n",
        "\n"
      ],
      "metadata": {
        "id": "JJajbzW0p2n-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN5-ZInqpjCB"
      },
      "outputs": [],
      "source": [
        "# 1. List page traversal\n",
        "\n",
        "# Navigate to the IMDB list page being used with Selenium\n",
        "url = \"https://www.imdb.com/search/title/?title_type=feature,tv_movie&release_date=2010-01-01,2022-01-01&languages=en&adult=include&sort=release_date,asc&start=0&ref_=adv_nxt\"\n",
        "driver.get(url) \n",
        "\n",
        "# Dictionary to save scraped data\n",
        "initial_link_dictionary={'movie_title':[],'movie_link':[],'year':[]}\n",
        "\n",
        "# Go through 1700 pages (feel free to change this value) of the list\n",
        "# Each loop collects the URL's for the films on that page\n",
        "for page_idx in range(0,1700):\n",
        "   \n",
        "  # Scrape the information of all films on the current page\n",
        "  film_content = driver.find_elements_by_class_name('lister-item')\n",
        "\n",
        "  # Process content of all 50 films on the page\n",
        "  for film_index in range(0,50):\n",
        "\n",
        "      try:\n",
        "\n",
        "        # Extract the index in front of the title. This is required for processing the title text\n",
        "        film_order = film_content[film_index].find_element_by_class_name('lister-item-index').text\n",
        "        # Get the year in which the film was released\n",
        "        film_year = film_content[film_index].find_element_by_class_name('lister-item-year').text\n",
        "        \n",
        "        # Get film title. The title requires some text processing\n",
        "        film_title = film_content[film_index].find_element_by_class_name('lister-item-header').text\n",
        "        # Remove the index and year from the film's title\n",
        "        film_title = film_title.replace(\" \"+film_year,\"\")\n",
        "        film_title = film_title.replace(film_order+' ', '')\n",
        "\n",
        "        # Scrape the link to the films IMDB page\n",
        "        film_link = film_content[film_index].find_element_by_link_text(film_title).get_attribute('href')\n",
        "        \n",
        "        # Update dictionary of scraped data\n",
        "        initial_link_dictionary['movie_title'].append(ftitle)\n",
        "        initial_link_dictionary['year'].append(fyear[fyear.find('2'):-1])\n",
        "        initial_link_dictionary['movie_link'].append(flink)\n",
        "\n",
        "      except:\n",
        "\n",
        "        # Catch inconsistencies in film naming convention (extremely rare)\n",
        "        continue \n",
        "    \n",
        "  # Get location of next page button on page\n",
        "  load_more = driver.find_element_by_class_name('next-page')\n",
        "  # Click on that button to traverse the pages of the list\n",
        "  load_more.click()\n",
        "\n",
        "# Create a data frame from the scraped data and save it as a CSV\n",
        "movie_url_df=pd.DataFrame(data=initial_link_dictionary)\n",
        "movie_url_df.to_csv('movie_urls.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9El1voF9ZDhr"
      },
      "outputs": [],
      "source": [
        "# If you already have a correctly formatted movie url list, load it in here\n",
        "movie_url_df=pd.read_csv('movie_urls.csv')\n",
        "movie_url_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpTXD_8FuEV1"
      },
      "outputs": [],
      "source": [
        "# 2. Get review page links from each films IMDB page\n",
        "\n",
        "# Dictionary to save scraped data\n",
        "final_link_dictionary={'movie_title':[],'movie_link':[],'review_link':[],'year':[]}\n",
        "\n",
        "# Loop through the links of each film, scraped in the last step\n",
        "for i,row in movie_url_df.iterrows():\n",
        "\n",
        "  # Parse information from data frame\n",
        "  url = row['movie_link']\n",
        "  movie_title = row['movie_title']\n",
        "  year = row['year']\n",
        "\n",
        "  try:\n",
        "\n",
        "    # Load the data from the current films IMDB page\n",
        "    page_data = requests.get(url, headers = {'User-Agent': 'Requests'})\n",
        "    # Parse the data\n",
        "    soup = BeautifulSoup(page_data.text, 'html.parser')\n",
        "\n",
        "    # Modify the link to the films IMDB page to get the link to the user review page\n",
        "    review_link=url.split('/')\n",
        "    review_link[-1]=soup.find('a', text = 'User reviews').get('href')\n",
        "    review_link='/'.join(review_link)\n",
        "\n",
        "    # Update dictionary of scraped data\n",
        "    final_link_dictionary['movie_title'].append(movie_title)\n",
        "    final_link_dictionary['movie_link'].append(url)\n",
        "    final_link_dictionary['year'].append(year)\n",
        "    final_link_dictionary['review_link'].append(review_link)\n",
        "\n",
        "  except:\n",
        "\n",
        "    # For movies with no user reviews, and hense no review link to extract\n",
        "    continue \n",
        "\n",
        "# Create a data frame from the scraped data and save it as a CSV\n",
        "review_url_df=pd.DataFrame(data=final_link_dictionary)\n",
        "review_url_df.to_csv('review_urls.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h8nQgXrkRQ8b"
      },
      "outputs": [],
      "source": [
        "# If you already have a correctly formatted movie review url list, load it in here\n",
        "review_url_df=pd.read_csv('review_urls.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMBv9IhQYpk_",
        "outputId": "7388a6e0-87fb-4828-b36b-e5fab3ea1f5f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
            "/usr/local/lib/python3.7/dist-packages/selenium/webdriver/remote/webelement.py:446: UserWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
            "  warnings.warn(\"find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70.2247191011236% complete\n",
            "70.34684904738641% complete\n",
            "70.46897899364924% complete\n",
            "70.59110893991206% complete\n",
            "70.71323888617489% complete\n",
            "70.83536883243772% complete\n",
            "70.95749877870054% complete\n",
            "71.07962872496336% complete\n",
            "71.20175867122619% complete\n",
            "71.323888617489% complete\n",
            "71.44601856375184% complete\n",
            "71.56814851001465% complete\n",
            "71.56814851001465% complete\n",
            "71.69027845627748% complete\n",
            "71.69027845627748% complete\n",
            "71.81240840254031% complete\n",
            "71.81240840254031% complete\n",
            "71.93453834880313% complete\n",
            "71.93453834880313% complete\n",
            "72.05666829506595% complete\n",
            "72.05666829506595% complete\n",
            "72.17879824132878% complete\n",
            "72.17879824132878% complete\n",
            "72.3009281875916% complete\n",
            "72.3009281875916% complete\n",
            "72.42305813385443% complete\n",
            "72.42305813385443% complete\n",
            "72.54518808011724% complete\n",
            "72.54518808011724% complete\n",
            "72.66731802638007% complete\n",
            "72.66731802638007% complete\n",
            "72.78944797264289% complete\n",
            "72.78944797264289% complete\n",
            "72.9115779189057% complete\n",
            "72.9115779189057% complete\n",
            "73.03370786516854% complete\n",
            "73.03370786516854% complete\n",
            "73.15583781143137% complete\n",
            "73.15583781143137% complete\n",
            "73.27796775769419% complete\n",
            "73.27796775769419% complete\n",
            "73.40009770395702% complete\n",
            "73.40009770395702% complete\n",
            "73.52222765021983% complete\n",
            "73.52222765021983% complete\n",
            "Checkpoint taken\n",
            "Checkpoint taken\n",
            "73.64435759648266% complete\n",
            "73.64435759648266% complete\n",
            "73.76648754274548% complete\n",
            "73.76648754274548% complete\n",
            "73.8886174890083% complete\n",
            "73.8886174890083% complete\n",
            "74.01074743527113% complete\n",
            "74.01074743527113% complete\n",
            "74.13287738153396% complete\n",
            "74.13287738153396% complete\n",
            "74.25500732779678% complete\n",
            "74.25500732779678% complete\n",
            "74.3771372740596% complete\n",
            "74.3771372740596% complete\n",
            "74.49926722032242% complete\n",
            "74.49926722032242% complete\n",
            "74.62139716658524% complete\n",
            "74.62139716658524% complete\n",
            "74.74352711284807% complete\n",
            "74.74352711284807% complete\n",
            "74.86565705911089% complete\n",
            "74.86565705911089% complete\n",
            "74.98778700537372% complete\n",
            "74.98778700537372% complete\n",
            "75.10991695163655% complete\n",
            "75.10991695163655% complete\n",
            "75.23204689789937% complete\n",
            "75.23204689789937% complete\n",
            "75.3541768441622% complete\n",
            "75.3541768441622% complete\n",
            "75.47630679042501% complete\n",
            "75.47630679042501% complete\n",
            "75.59843673668783% complete\n",
            "75.59843673668783% complete\n",
            "75.72056668295066% complete\n",
            "75.72056668295066% complete\n",
            "75.84269662921348% complete\n",
            "75.84269662921348% complete\n",
            "75.96482657547631% complete\n",
            "75.96482657547631% complete\n",
            "76.08695652173914% complete\n",
            "76.08695652173914% complete\n",
            "76.20908646800196% complete\n",
            "76.20908646800196% complete\n",
            "76.33121641426477% complete\n",
            "76.33121641426477% complete\n",
            "76.45334636052759% complete\n",
            "76.45334636052759% complete\n",
            "76.57547630679042% complete\n",
            "76.57547630679042% complete\n",
            "76.69760625305325% complete\n",
            "76.69760625305325% complete\n",
            "76.81973619931607% complete\n",
            "76.81973619931607% complete\n",
            "76.9418661455789% complete\n",
            "76.9418661455789% complete\n",
            "Checkpoint taken\n",
            "Checkpoint taken\n",
            "77.06399609184173% complete\n",
            "77.06399609184173% complete\n",
            "77.18612603810455% complete\n",
            "77.18612603810455% complete\n",
            "77.30825598436736% complete\n",
            "77.30825598436736% complete\n",
            "77.43038593063018% complete\n",
            "77.43038593063018% complete\n",
            "77.55251587689301% complete\n",
            "77.55251587689301% complete\n",
            "77.67464582315584% complete\n",
            "77.67464582315584% complete\n",
            "77.79677576941866% complete\n",
            "77.79677576941866% complete\n",
            "77.91890571568149% complete\n",
            "77.91890571568149% complete\n",
            "78.0410356619443% complete\n",
            "78.0410356619443% complete\n",
            "78.16316560820712% complete\n",
            "78.16316560820712% complete\n",
            "78.28529555446995% complete\n",
            "78.28529555446995% complete\n",
            "78.40742550073277% complete\n",
            "78.40742550073277% complete\n",
            "78.5295554469956% complete\n",
            "78.5295554469956% complete\n",
            "78.65168539325843% complete\n",
            "78.65168539325843% complete\n",
            "78.77381533952125% complete\n",
            "78.77381533952125% complete\n",
            "78.89594528578408% complete\n",
            "78.89594528578408% complete\n",
            "79.0180752320469% complete\n",
            "79.0180752320469% complete\n",
            "79.14020517830971% complete\n",
            "79.14020517830971% complete\n",
            "79.26233512457254% complete\n",
            "79.26233512457254% complete\n",
            "79.38446507083536% complete\n",
            "79.38446507083536% complete\n",
            "79.50659501709819% complete\n",
            "79.50659501709819% complete\n",
            "79.62872496336102% complete\n",
            "79.62872496336102% complete\n",
            "79.75085490962384% complete\n",
            "79.75085490962384% complete\n",
            "79.87298485588667% complete\n",
            "79.87298485588667% complete\n",
            "79.99511480214949% complete\n",
            "79.99511480214949% complete\n",
            "80.1172447484123% complete\n",
            "80.1172447484123% complete\n",
            "80.23937469467513% complete\n",
            "80.23937469467513% complete\n",
            "80.36150464093795% complete\n",
            "80.36150464093795% complete\n",
            "80.48363458720078% complete\n",
            "80.48363458720078% complete\n",
            "Checkpoint taken\n",
            "Checkpoint taken\n",
            "80.60576453346361% complete\n",
            "80.60576453346361% complete\n",
            "80.72789447972643% complete\n",
            "80.72789447972643% complete\n",
            "80.85002442598925% complete\n",
            "80.85002442598925% complete\n",
            "80.97215437225208% complete\n",
            "80.97215437225208% complete\n",
            "81.0942843185149% complete\n",
            "81.0942843185149% complete\n",
            "81.21641426477773% complete\n",
            "81.21641426477773% complete\n",
            "81.33854421104054% complete\n",
            "81.33854421104054% complete\n",
            "81.46067415730337% complete\n",
            "81.46067415730337% complete\n",
            "81.5828041035662% complete\n",
            "81.5828041035662% complete\n",
            "81.70493404982902% complete\n",
            "81.70493404982902% complete\n",
            "81.82706399609184% complete\n",
            "81.82706399609184% complete\n",
            "81.94919394235467% complete\n",
            "81.94919394235467% complete\n",
            "82.07132388861748% complete\n",
            "82.07132388861748% complete\n",
            "82.19345383488032% complete\n",
            "82.19345383488032% complete\n",
            "82.31558378114313% complete\n",
            "82.31558378114313% complete\n",
            "82.43771372740596% complete\n",
            "82.43771372740596% complete\n",
            "82.55984367366878% complete\n",
            "82.55984367366878% complete\n",
            "82.6819736199316% complete\n",
            "82.6819736199316% complete\n",
            "82.80410356619443% complete\n",
            "82.80410356619443% complete\n",
            "82.92623351245726% complete\n",
            "82.92623351245726% complete\n",
            "83.04836345872008% complete\n",
            "83.04836345872008% complete\n",
            "83.1704934049829% complete\n",
            "83.1704934049829% complete\n",
            "83.29262335124572% complete\n",
            "83.29262335124572% complete\n",
            "83.41475329750855% complete\n",
            "83.41475329750855% complete\n",
            "83.53688324377137% complete\n",
            "83.53688324377137% complete\n",
            "83.65901319003419% complete\n",
            "83.65901319003419% complete\n",
            "83.78114313629702% complete\n",
            "83.78114313629702% complete\n",
            "83.90327308255985% complete\n",
            "83.90327308255985% complete\n",
            "Checkpoint taken\n",
            "Checkpoint taken\n",
            "84.02540302882267% complete\n",
            "84.02540302882267% complete\n",
            "84.1475329750855% complete\n",
            "84.1475329750855% complete\n",
            "84.26966292134831% complete\n",
            "84.26966292134831% complete\n",
            "84.39179286761114% complete\n",
            "84.39179286761114% complete\n",
            "84.51392281387396% complete\n",
            "84.51392281387396% complete\n",
            "84.63605276013678% complete\n",
            "84.63605276013678% complete\n",
            "84.75818270639961% complete\n",
            "84.75818270639961% complete\n",
            "84.88031265266244% complete\n",
            "84.88031265266244% complete\n",
            "85.00244259892526% complete\n",
            "85.00244259892526% complete\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "#Set list for each element:\n",
        "title = []\n",
        "content = []\n",
        "rating = []\n",
        "date = []\n",
        "movie_title=[]\n",
        "# Step 2, we will grab the data from each user review page\n",
        "# Use Selenium to go to each user review page. each page is for a different film\n",
        "checkpoint=time.time()\n",
        "for i in range(len(review_url_df['review_link'])):\n",
        "\n",
        "  if i<=11486:\n",
        "    continue\n",
        "  \n",
        "  if (i%20)==0:\n",
        "    print(f'{(i/16376)*100}% complete')\n",
        "\n",
        "  if(time.time()-checkpoint)>1800:\n",
        "    data = {'review_title': title, \n",
        "    'review_rating': rating,\n",
        "    'review_date' : date,\n",
        "    'review_body' : content,\n",
        "    'movie_title': movie_title\n",
        "    }\n",
        "  #Build dataframe for each movie to export\n",
        "    review = pd.DataFrame(data = data)\n",
        "    review.to_csv(f'review_data_11486_to_{i}.csv')\n",
        "    print('Checkpoint taken')\n",
        "    checkpoint=time.time()\n",
        "\n",
        "  driver.get(review_url_df['review_link'][i]) # Go to user review page\n",
        "  #driver.implicitly_wait(1) # tell the webdriver to wait for 1 seconds for the page to load to prevent blocked by anti spam software\n",
        "  \n",
        "  current_title=review_url_df['movie_title'][i]\n",
        "\n",
        "  # LOAD MORE REVIEWS FOR A GIVEN FILM. once loaded, can save reviews\n",
        "  # Set up action to click on 'load more' button\n",
        "  # note that each page on imdb has 25 reviews\n",
        "  page = 1 #Set initial variable for while loop\n",
        "  #We want at least 1000 review, so get 50 at a safe number\n",
        "  while page<5:  \n",
        "      try:\n",
        "          #find the load more button on the webpage\n",
        "          load_more = driver.find_element_by_id('load-more-trigger')\n",
        "          #click on that button\n",
        "          load_more.click()\n",
        "          page+=1 #move on to next loadmore button\n",
        "      except:\n",
        "          #If couldnt find any button to click, stop\n",
        "          break\n",
        "  # After fully expand the page, we will grab data from whole website\n",
        "  review = driver.find_elements_by_class_name('review-container')\n",
        "\n",
        "  # save reviews for a given film\n",
        "  for n in range(0,125):\n",
        "      try:\n",
        "          #Some reviewers only give review text or rating without the other, \n",
        "          #so we use try/except here to make sure each block of content must has all the element before append them to the list\n",
        "\n",
        "          #Check if each review has all the elements\n",
        "          ftitle = review[n].find_element_by_class_name('title').text\n",
        "          #For the review content, some of them are hidden as spoiler, \n",
        "          #so we use the attribute 'textContent' here after extracting the 'content' tag\n",
        "          fcontent = review[n].find_element_by_class_name('content').get_attribute(\"textContent\").strip()\n",
        "          frating = review[n].find_element_by_class_name('rating-other-user-rating').text\n",
        "          fdate = review[n].find_element_by_class_name('review-date').text\n",
        "\n",
        "          #Then add them to the respective list\n",
        "          title.append(ftitle)\n",
        "          content.append(fcontent)\n",
        "          rating.append(frating.split('/')[0])\n",
        "          date.append(fdate)\n",
        "          movie_title.append(current_title)\n",
        "      except:\n",
        "          continue\n",
        "\n",
        "#Build data dictionary for dataframe\n",
        "data = {'review_title': title, \n",
        "    'review_rating': rating,\n",
        "    'review_date' : date,\n",
        "    'review_body' : content,\n",
        "    'movie_title': movie_title\n",
        "    }\n",
        "#Build dataframe for each movie to export\n",
        "review = pd.DataFrame(data = data)\n",
        "review.to_csv(f'review_data.csv')\n",
        "#movie = top50['Movie_name'][i] #grab the movie name from the top50 list    \n",
        "#review['Movie_name'] = movie #create new column with the same movie name column    \n",
        "#review.to_csv(f'data/{folder_name}/{i+1}.csv') #store them into individual file for each movies, so we can combine or check them later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYsV1DDxxHMW"
      },
      "outputs": [],
      "source": [
        "review.to_csv(f'review_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "655PJ-k3cy7A",
        "outputId": "fb56d223-141d-4b3c-cf0a-01a359f36779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.246926497515041\n"
          ]
        }
      ],
      "source": [
        "sum=0\n",
        "for i in range(len(review['Review Rating'])):\n",
        "  sum+=int(review['Review Rating'][i].split('/')[0])\n",
        "average_score=sum/len(review['Review Rating'])\n",
        "print(average_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "W_oM2RY5hPBp",
        "outputId": "26f8f2d3-6fa2-4995-e5da-7b74b9ac5391"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"7 October 2016. Yes, this movie is predictable, the storyline nothing special that hasn't already been done before. Yet, the music is really sharp, crisp, and engrossingly fun and hip. The performances are subtly different and more natural and easy-going without overly drama. This Disney Version of the famous pop star accidentally meets unknown, plain girl is done in an appealing and straightforward, honest way that doesn't depend on anything more than a good music track, sung well, and performances and chemistry which are entertaining and smartly done. Other fun movies include I'll Be There (2003), A Royal Night Out (2015), Roman Holiday (1953), Music and Lyrics (2007), Pride and Prejudice (2005), The Artist (2011), The Devil Wears Prada (2006), Good Morning Call (2016),\\n                \\n                    1 out of 1 found this helpful.\\n                        \\n                            Was this review helpful?  Sign in to vote.\\n                        \\n                        \\n                    Permalink\""
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "review['Review_body'][83]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "web_scraper.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLYloRt2V1GL34wf2j2Q+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}