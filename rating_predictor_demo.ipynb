{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rating_predictor_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XwnqYtidM3LA"
      ],
      "authorship_tag": "ABX9TyNEB509v0hKA44NWzn2Ds1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaikadish/imdbProject/blob/main/rating_predictor_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to my Review Rating Predictor demo!\n",
        "\n",
        "This is a demo for an NLP model I trained to predict your rating of a movie (on a scale from 1-10) based on a review that you write for it. \n",
        "\n",
        "To run the demo, hover your mouse over the code block directly below the **Demo** heading and press the play button on the top left. The first time you run the demo, some files will be loaded, which can take a minute or two. Once the demo is running, you can type your film review in the box that appears at the bottom of the page, and then press enter to see the score that the model predicted. To run the demo again, just press the play button! Remember, the model performs best if you type longer reviews, so if it is giving nonsensical ratings from reviews like \"That movie sucked\" try fleshing out the review to get a more accurate rating.\n",
        "\n",
        "If you are interested in how I made this, please check out this [tutorial](https://github.com/shaikadish/imdbProject) I wrote which details the entire process!"
      ],
      "metadata": {
        "id": "XwnqYtidM3LA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo:"
      ],
      "metadata": {
        "id": "segNszwYKiIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0RzydEXU90FM",
        "outputId": "127f936c-082a-4130-fadb-db9681e72e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-efa6efba4a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Run demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mreview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type a review: \\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nPredicted rating: {prediction}/10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# I am remove all printed output. I don't want to scare any non-technical people!\n",
        "\n",
        "# Run setup only on first run\n",
        "if not('BertTokenizer' in locals()):\n",
        "  # Download model and library\n",
        "  print('Downloading important things...')\n",
        "  !pip install transformers &> /dev/null\n",
        "  !gdown --id 1-00HNjkWgzq6_b-UDJtUfztR5Ga-Ymbi &> /dev/null\n",
        "\n",
        "  # Disable output watnings\n",
        "  import warnings\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "  # Import libraries\n",
        "  from transformers.utils.logging import disable_progress_bar\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  from transformers import BertTokenizer, AutoModel\n",
        "  import transformers\n",
        "  transformers.logging.set_verbosity_error()\n",
        "\n",
        "  # Model class\n",
        "  class RatingModel(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(RatingModel, self).__init__()\n",
        "          \n",
        "          # Load pretrained BERT model\n",
        "          self.base_model = AutoModel.from_pretrained('fabriceyhc/bert-base-uncased-imdb')\n",
        "          # Include dropout layer in classification head\n",
        "          self.dropout = nn.Dropout(0.5)\n",
        "          # Add new output layer for classification\n",
        "          self.linear = nn.Linear(768, 1) \n",
        "          \n",
        "      def forward(self, tokenized_reviews, attn_mask):\n",
        "\n",
        "          # Pass the tokenized reviews through the BERT model\n",
        "          outputs = self.base_model(tokenized_reviews, attention_mask=attn_mask)\n",
        "          outputs = self.dropout(outputs[0])\n",
        "\n",
        "          # The slicing of the BERT model outputs (bellow) is to make use of the BERT features \n",
        "          # from the only the special [CLS] token for classification. This token is at index 0\n",
        "          outputs = self.linear(outputs[:,0,:])\n",
        "          \n",
        "          return outputs\n",
        "\n",
        "  # Predictor class\n",
        "  class RatingPredictor():\n",
        "      def __init__(self,model,tokenizer):\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        self.model = model.eval()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "      def predict(self,review):\n",
        "\n",
        "        # Tokenize review\n",
        "        encoded_dict = self.tokenizer.encode_plus(\n",
        "                          review,                      \n",
        "                          truncation=True,\n",
        "                          add_special_tokens = True, \n",
        "                          max_length = 512,          \n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                                            )\n",
        "            \n",
        "        tokenized_review = encoded_dict['input_ids']\n",
        "        attention_mask = encoded_dict['attention_mask']\n",
        "\n",
        "        # Get rating prediction\n",
        "        preds = self.model(torch.tensor([tokenized_review]),torch.tensor([attention_mask]))\n",
        "\n",
        "        # Return rating out of 10\n",
        "        return round(preds.item()*10)\n",
        "\n",
        "  # Download tokenizer\n",
        "  print('Almost done...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('fabriceyhc/bert-base-uncased-imdb', do_lower_case=True,disable_progress_bar=True)\n",
        "\n",
        "  # Create model and load\n",
        "  model = RatingModel()\n",
        "  model.load_state_dict(torch.load('imdb_1.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "  # Create predictor\n",
        "  predictor = RatingPredictor(model,tokenizer)\n",
        "  print('Done!\\n')\n",
        "\n",
        "# Run demo\n",
        "review=input(\"Type a review: \\n\\n\")\n",
        "prediction=predictor.predict(review)\n",
        "print(f'\\nPredicted rating: {prediction}/10')"
      ]
    }
  ]
}