{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaikadish/imdbProject/blob/main/web_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are we doing here?\n",
        "\n",
        "Although [brilliant datasets](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) exist for sentiment analysis using IMDB user reviews, you might need something a little different. \n",
        "\n",
        "For example, the project I am working on is to try and estimate the rating out of ten a user would give a film, based on their review of the film. I struggled to find a dataset that had exactly what I needed (user reviews and their ratings), so I decided to make my own. This notebook has the code I used to do that.\n",
        "\n",
        "I hope that this notebook can act as a tutorial for anyone interested in building their own dataset through the use of pretty standard web-scraping techniques.\n"
      ],
      "metadata": {
        "id": "sk-cbVbHfRUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and setup\n",
        "\n",
        "Firstly, I am working off colabs, so there is some code here just to mount to your google drive, and to import the appropriate python libraries. The main libraries we will be using here are: \n",
        "*   [Selenium](https://selenium-python.readthedocs.io/) to automate web browsing activities.\n",
        "*   [requests](https://docs.python-requests.org/en/latest/) for reading the *HTML* file on a given page.\n",
        "*   [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) for parsing the *HTML* files.\n",
        "*   [Pandas](https://pandas.pydata.org/docs/) for tabular data manipulation and saving."
      ],
      "metadata": {
        "id": "2nTxApuahhxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjK7_udlZyBX"
      },
      "outputs": [],
      "source": [
        "# Mount to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "%cd drive/MyDrive/GitHub/IMDB_project/imdbProject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR2TXAN0nvGL"
      },
      "outputs": [],
      "source": [
        "# Install libraries to colab environment\n",
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GyN5wMyUpMKN"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import selenium\n",
        "import requests \n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnnoIS6AsiPw"
      },
      "outputs": [],
      "source": [
        "# Configure Selenium webdriver for colab notebook\n",
        "from selenium import webdriver\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping\n",
        "\n",
        "Now comes the good stuff. We will get access to the review data in three steps:\n",
        "\n",
        "\n",
        "1.   This step automates the traversal of an [*IMDB list page*](https://www.imdb.com/search/title/?title_type=feature,tv_movie&release_date=2010-01-01,2022-01-01&languages=en&adult=include&sort=release_date,asc&start=0&ref_=adv_nxt), where the set of films can be found. The URL to each of these films [*IMDB page*](https://www.imdb.com/title/tt0368226/?ref_=nv_sr_srsg_0) are extracted through this traversal.\n",
        "2.   Next, each of these film's *IMDB Page*s are visited, and the URL to that film's [review page](https://www.imdb.com/title/tt0368226/reviews?ref_=tt_urv) is scraped.\n",
        "3.   Finally, the user review and ratings are scraped from each of review page.\n",
        "\n",
        "This process is broken into these steps because it is less convoluted to automate each process individually than to try do them all at once. This approach was also used for those of you are here to copy some code, as each of these steps can easily be modified for similar projects, abd the modularity helps with code editing.\n",
        "\n"
      ],
      "metadata": {
        "id": "JJajbzW0p2n-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN5-ZInqpjCB"
      },
      "outputs": [],
      "source": [
        "# 1. List page traversal\n",
        "\n",
        "# Navigate to the IMDB list page being used with Selenium\n",
        "url = \"https://www.imdb.com/search/title/?title_type=feature,tv_movie&release_date=2010-01-01,2022-01-01&languages=en&adult=include&sort=release_date,asc&start=0&ref_=adv_nxt\"\n",
        "driver.get(url) \n",
        "\n",
        "# Dictionary to save scraped data\n",
        "initial_link_dictionary={'movie_title':[],'movie_link':[],'year':[]}\n",
        "\n",
        "# Go through 1700 pages (feel free to change this value) of the list\n",
        "# Each loop collects the URL's for the films on that page\n",
        "for page_idx in range(0,1700):\n",
        "   \n",
        "  # Scrape the information of all films on the current page\n",
        "  film_content = driver.find_elements_by_class_name('lister-item')\n",
        "\n",
        "  # Process content of all 50 films on the page\n",
        "  for film_index in range(0,50):\n",
        "\n",
        "      try:\n",
        "\n",
        "        # Extract the index in front of the title. This is required for processing the title text\n",
        "        film_order = film_content[film_index].find_element_by_class_name('lister-item-index').text\n",
        "        # Get the year in which the film was released\n",
        "        film_year = film_content[film_index].find_element_by_class_name('lister-item-year').text\n",
        "        \n",
        "        # Get film title. The title requires some text processing\n",
        "        film_title = film_content[film_index].find_element_by_class_name('lister-item-header').text\n",
        "        # Remove the index and year from the film's title\n",
        "        film_title = film_title.replace(\" \"+film_year,\"\")\n",
        "        film_title = film_title.replace(film_order+' ', '')\n",
        "\n",
        "        # Scrape the link to the films IMDB page\n",
        "        film_link = film_content[film_index].find_element_by_link_text(film_title).get_attribute('href')\n",
        "        \n",
        "        # Update dictionary of scraped data\n",
        "        initial_link_dictionary['movie_title'].append(ftitle)\n",
        "        initial_link_dictionary['year'].append(fyear[fyear.find('2'):-1])\n",
        "        initial_link_dictionary['movie_link'].append(flink)\n",
        "\n",
        "      except:\n",
        "\n",
        "        # Catch inconsistencies in film naming convention (extremely rare)\n",
        "        continue \n",
        "    \n",
        "  # Get location of next page button on page\n",
        "  load_more = driver.find_element_by_class_name('next-page')\n",
        "  # Click on that button to traverse the pages of the list\n",
        "  load_more.click()\n",
        "\n",
        "# Create a data frame from the scraped data and save it as a CSV\n",
        "movie_url_df=pd.DataFrame(data=initial_link_dictionary)\n",
        "movie_url_df.to_csv('movie_urls.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9El1voF9ZDhr"
      },
      "outputs": [],
      "source": [
        "# If you already have a correctly formatted movie url list, load it in here\n",
        "movie_url_df=pd.read_csv('movie_urls.csv')\n",
        "movie_url_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpTXD_8FuEV1"
      },
      "outputs": [],
      "source": [
        "# 2. Get review page links from each films IMDB page\n",
        "\n",
        "# Dictionary to save scraped data\n",
        "final_link_dictionary={'movie_title':[],'movie_link':[],'review_link':[],'year':[]}\n",
        "\n",
        "# Loop through the links of each film, scraped in the last step\n",
        "for i,row in movie_url_df.iterrows():\n",
        "\n",
        "  # Parse information from data frame\n",
        "  url = row['movie_link']\n",
        "  movie_title = row['movie_title']\n",
        "  year = row['year']\n",
        "\n",
        "  try:\n",
        "\n",
        "    # Load the data from the current films IMDB page\n",
        "    page_data = requests.get(url, headers = {'User-Agent': 'Requests'})\n",
        "    # Parse the data\n",
        "    soup = BeautifulSoup(page_data.text, 'html.parser')\n",
        "\n",
        "    # Modify the link to the films IMDB page to get the link to the user review page\n",
        "    review_link=url.split('/')\n",
        "    review_link[-1]=soup.find('a', text = 'User reviews').get('href')\n",
        "    review_link='/'.join(review_link)\n",
        "\n",
        "    # Update dictionary of scraped data\n",
        "    final_link_dictionary['movie_title'].append(movie_title)\n",
        "    final_link_dictionary['movie_link'].append(url)\n",
        "    final_link_dictionary['year'].append(year)\n",
        "    final_link_dictionary['review_link'].append(review_link)\n",
        "\n",
        "  except:\n",
        "\n",
        "    # For movies with no user reviews, and hense no review link to extract\n",
        "    continue \n",
        "\n",
        "# Create a data frame from the scraped data and save it as a CSV\n",
        "review_url_df=pd.DataFrame(data=final_link_dictionary)\n",
        "review_url_df.to_csv('review_urls.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h8nQgXrkRQ8b"
      },
      "outputs": [],
      "source": [
        "# If you already have a correctly formatted movie review url list, load it in here\n",
        "review_url_df=pd.read_csv('review_urls.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMBv9IhQYpk_"
      },
      "outputs": [],
      "source": [
        "# 3. Scrape user reviews and ratings from film review pages\n",
        "\n",
        "# Dictionary to save scraped data\n",
        "review_dictionary={'review_title':[],'review_rating':[],'review_date':[],'review_body':[],'movie_title':[]}\n",
        "\n",
        "# Loop through each film's review page\n",
        "for i in range(len(review_url_df['review_link'])):\n",
        "\n",
        "  # Go to review page of current film\n",
        "  driver.get(review_url_df['review_link'][i]) # Go to user review page\n",
        "\n",
        "  # Store title of current film  \n",
        "  current_title=review_url_df['movie_title'][i]\n",
        "\n",
        "  # Expand review list on current page 5 times using Selenium\n",
        "  page = 1 \n",
        "  while page<5:  \n",
        "\n",
        "      try:\n",
        "          # Find the button to load more reviews\n",
        "          load_more = driver.find_element_by_id('load-more-trigger')\n",
        "          # Click the button\n",
        "          load_more.click()\n",
        "          page+=1\n",
        "\n",
        "      except:\n",
        "          # If no more reviews to load, break\n",
        "          break\n",
        "          \n",
        "  # Grab all reviews from fully expanded list\n",
        "  reviews = driver.find_elements_by_class_name('review-container')\n",
        "\n",
        "  # Process reviews for the current film\n",
        "  for review in reviews:\n",
        "\n",
        "      try:\n",
        "        \n",
        "          # Get review information\n",
        "          review_title = review.find_element_by_class_name('title').text\n",
        "          # The \"textContent\" attribute gets passed the spoiler warning\n",
        "          review_content = review.find_element_by_class_name('content').get_attribute(\"textContent\").strip()\n",
        "          review_rating = review.find_element_by_class_name('rating-other-user-rating').text\n",
        "          review_date = review.find_element_by_class_name('review-date').text\n",
        "\n",
        "          # Update dictionary of sraped data\n",
        "          review_dictionary['review_title'].append(review_title)\n",
        "          review_dictionary['review_body'].append(review_content)\n",
        "          review_dictionary['review_rating'].append(review_rating.split('/')[0])\n",
        "          review_dictionary['review_date'].append(review_date)\n",
        "          review_dictionary['movie_title'].append(current_title)\n",
        "\n",
        "      except:\n",
        "\n",
        "          # Some reviews do not have ratings. These reviews must be skipped\n",
        "          continue\n",
        "\n",
        "# Create a data frame from the scraped data and save it as a CSV\n",
        "review_df = pd.DataFrame(data = review_dictionary)\n",
        "review_df.to_csv('review_data.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there you have it! If you are interested in the analysis and use of this data, please feel free to checkout the other notebooks on this repo!"
      ],
      "metadata": {
        "id": "ZCjo3SNutHe0"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2nTxApuahhxh",
        "JJajbzW0p2n-"
      ],
      "name": "web_scraper.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNihqbD5zxb15sl7YeuOV2V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}