{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rating_predictor_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XwnqYtidM3LA"
      ],
      "authorship_tag": "ABX9TyMFJneoigj4hQ09W9eYyyBg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaikadish/imdbProject/blob/main/rating_predictor_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to my Review Rating Predictor demo!\n",
        "\n",
        "Before we run the demo, we need to load in important files. Hover the mouse over the code block directly bellow, and press the play button in the top left. This can take a minute or two. Scroll down to the **Demo** heading to run the demo.\n"
      ],
      "metadata": {
        "id": "XwnqYtidM3LA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0RzydEXU90FM"
      },
      "outputs": [],
      "source": [
        "# Remove all printed output. I don't want to scare any non-technical people!\n",
        "%%capture\n",
        "\n",
        "# Download model and library\n",
        "!pip install transformers\n",
        "!gdown --id 1-00HNjkWgzq6_b-UDJtUfztR5Ga-Ymbi\n",
        "\n",
        "# Disable output watnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Import libraties\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, AutoModel\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "# Model class\n",
        "class RatingModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RatingModel, self).__init__()\n",
        "        \n",
        "        # Load pretrained BERT model\n",
        "        self.base_model = AutoModel.from_pretrained('fabriceyhc/bert-base-uncased-imdb')\n",
        "        # Include dropout layer in classification head\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # Add new output layer for classification\n",
        "        self.linear = nn.Linear(768, 1) \n",
        "        \n",
        "    def forward(self, tokenized_reviews, attn_mask):\n",
        "\n",
        "        # Pass the tokenized reviews through the BERT model\n",
        "        outputs = self.base_model(tokenized_reviews, attention_mask=attn_mask)\n",
        "        outputs = self.dropout(outputs[0])\n",
        "\n",
        "        # The slicing of the BERT model outputs (bellow) is to make use of the BERT features \n",
        "        # from the only the special [CLS] token for classification. This token is at index 0\n",
        "        outputs = self.linear(outputs[:,0,:])\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "# Predictor class\n",
        "class RatingPredictor():\n",
        "    def __init__(self,model,tokenizer):\n",
        "\n",
        "      # Load model and tokenizer\n",
        "      self.model = model.eval()\n",
        "      self.tokenizer = tokenizer\n",
        "\n",
        "    def predict(self,review):\n",
        "\n",
        "      # Tokenize review\n",
        "      encoded_dict = self.tokenizer.encode_plus(\n",
        "                        review,                      \n",
        "                        truncation=True,\n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 512,          \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                                           )\n",
        "          \n",
        "      tokenized_review=encoded_dict['input_ids']\n",
        "      attention_mask=encoded_dict['attention_mask']\n",
        "\n",
        "      # Get rating prediction\n",
        "      preds=self.model(torch.tensor([tokenized_review]),torch.tensor([attention_mask]))\n",
        "\n",
        "      # Return rating out of 10\n",
        "      return round(preds.item()*10)\n",
        "\n",
        "# Download tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('fabriceyhc/bert-base-uncased-imdb', do_lower_case=True)\n",
        "\n",
        "# Create model and load\n",
        "model = RatingModel()\n",
        "model.load_state_dict(torch.load('imdb_1.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "# Create predictor\n",
        "predictor = RatingPredictor(model,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo\n",
        "\n",
        "To run the demo, hover the mouse over the code block bellow, and press the play button. You can then type your review into the box that appears, and press enter to see the predicted rating. You can run this box as many times as you want! \n",
        "\n",
        "Remember, the model performs best if you type longer reviews, so if it is giving nonsensical ratings from reviews like \"That movie sucked\" try fleshing out the review to get a more accurate rating."
      ],
      "metadata": {
        "id": "6Ltz6sFONL6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review=input(\"Type a review: \\n\")\n",
        "prediction=predictor.predict(review)\n",
        "print(f'Predicted rating: {prediction}/10')"
      ],
      "metadata": {
        "id": "UvE8N17lJWm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are interested in how I made this, please check out this [tutorial](https://github.com/shaikadish/imdbProject) I wrote which details the entire process!"
      ],
      "metadata": {
        "id": "jcsj9We7QidN"
      }
    }
  ]
}